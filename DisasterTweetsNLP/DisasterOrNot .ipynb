{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting whether a tweet is about a disaster or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', dtype='<U32')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "np.array(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for i in string.punctuation:\n",
    "        if i in tokens:\n",
    "            tokens = list(filter(lambda x:x!=i,tokens))\n",
    "    return tokens        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove hyperlinks in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tokens):\n",
    "    indices = [i for i, x in enumerate(tokens) if x == \"http\"]\n",
    "    indices1 = [i for i, x in enumerate(tokens) if x == \"https\"]\n",
    "    a = []\n",
    "    for i in indices:\n",
    "        a.append(tokens[i+1])\n",
    "    for i in indices1:\n",
    "        a.append(tokens[i+1])\n",
    "    tokens = list(filter(lambda x:(x!='http') and (x!='https')and (x not in a) ,tokens))\n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "def stem_tokens(tokens, porter):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(porter.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(tokens):\n",
    "    for i in tokens:\n",
    "        if i.isdigit()==True:\n",
    "            tokens.remove(i)\n",
    "    return tokens        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    for i in tokens:\n",
    "        if i in stoplist_combined:\n",
    "            tokens.remove(i)\n",
    "    return tokens        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all above five functions into one tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    a = remove_stopwords(remove_digits(remove_links(remove_punc(text))))\n",
    "    #b  = stem_tokens(a,porter)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza reception accuracy = 80.03939592908733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=tokenizer,ngram_range=(1,1)\n",
    "                            ,min_df=0)\n",
    "#matrix = vectorizer.fit_transform(list(train['text']))\n",
    "from sklearn.model_selection import train_test_split\n",
    "x,y = train_test_split(train,test_size=0.2)\n",
    "\n",
    "x_train = vectorizer.fit_transform(list(x['text']))\n",
    "y_train = x['target']\n",
    "\n",
    "x_test = vectorizer.transform(list(y['text']))\n",
    "y_test = y['target']\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB() \n",
    "\n",
    "clf.fit(x_train,y_train) \n",
    "\n",
    "predictions_valid = clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Pizza reception accuracy = {}'.format(\n",
    "        accuracy_score(predictions_valid, y_test) * 100)\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "a  =train['text'].apply(tokenizer)\n",
    "b = test['text'].apply(tokenizer)\n",
    "df = pd.concat([a,b])\n",
    "corpus = np.array(df)\n",
    "\n",
    "model = Word2Vec(sentences=corpus,min_count=1,size = 100,window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "\n",
    "word_ind = tokenizer_obj.word_index\n",
    "\n",
    "matrix  = pd.DataFrame(np.zeros((len(word_ind)+1,100)))\n",
    "for word,i in word_ind.items():\n",
    "    matrix.loc[i,:] = model[word]\n",
    "    \n",
    "matrix = matrix.drop([0],axis=0)\n",
    "matrix = np.array(matrix)\n",
    "max_length = max([len(i) for i in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(len(word_ind),100,embeddings_initializer = Constant(matrix),input_length =max_length,trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 24, 100)           2312400   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,325,201\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 2,312,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_tokens = tokenizer_obj.texts_to_sequences(train['text'])\n",
    "x = pad_sequences(train_tokens,maxlen=max_length,padding='post')\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/25\n",
      " - 10s - loss: 0.7028 - accuracy: 0.5509 - val_loss: 0.6849 - val_accuracy: 0.5601\n",
      "Epoch 2/25\n",
      " - 6s - loss: 0.6832 - accuracy: 0.5522 - val_loss: 0.6854 - val_accuracy: 0.5601\n",
      "Epoch 3/25\n",
      " - 6s - loss: 0.6762 - accuracy: 0.5598 - val_loss: 0.6848 - val_accuracy: 0.5601\n",
      "Epoch 4/25\n",
      " - 6s - loss: 0.6735 - accuracy: 0.5647 - val_loss: 0.6830 - val_accuracy: 0.5601\n",
      "Epoch 5/25\n",
      " - 6s - loss: 0.6741 - accuracy: 0.5639 - val_loss: 0.6827 - val_accuracy: 0.5601\n",
      "Epoch 6/25\n",
      " - 7s - loss: 0.6729 - accuracy: 0.5681 - val_loss: 0.6837 - val_accuracy: 0.5601\n",
      "Epoch 7/25\n",
      " - 6s - loss: 0.6718 - accuracy: 0.5695 - val_loss: 0.6840 - val_accuracy: 0.5601\n",
      "Epoch 8/25\n",
      " - 7s - loss: 0.6694 - accuracy: 0.5721 - val_loss: 0.6871 - val_accuracy: 0.5489\n",
      "Epoch 9/25\n",
      " - 7s - loss: 0.6677 - accuracy: 0.5719 - val_loss: 0.6865 - val_accuracy: 0.5594\n",
      "Epoch 10/25\n",
      " - 7s - loss: 0.6663 - accuracy: 0.5750 - val_loss: 0.6831 - val_accuracy: 0.5601\n",
      "Epoch 11/25\n",
      " - 7s - loss: 0.6651 - accuracy: 0.5834 - val_loss: 0.6842 - val_accuracy: 0.5601\n",
      "Epoch 12/25\n",
      " - 8s - loss: 0.6649 - accuracy: 0.5900 - val_loss: 0.6834 - val_accuracy: 0.5601\n",
      "Epoch 13/25\n",
      " - 8s - loss: 0.6610 - accuracy: 0.5957 - val_loss: 0.6844 - val_accuracy: 0.5601\n",
      "Epoch 14/25\n",
      " - 8s - loss: 0.6600 - accuracy: 0.6015 - val_loss: 0.6864 - val_accuracy: 0.5752\n",
      "Epoch 15/25\n",
      " - 8s - loss: 0.6555 - accuracy: 0.6128 - val_loss: 0.6837 - val_accuracy: 0.5601\n",
      "Epoch 16/25\n",
      " - 8s - loss: 0.6588 - accuracy: 0.6094 - val_loss: 0.6834 - val_accuracy: 0.5601\n",
      "Epoch 17/25\n",
      " - 8s - loss: 0.6538 - accuracy: 0.6112 - val_loss: 0.6848 - val_accuracy: 0.5601\n",
      "Epoch 18/25\n",
      " - 8s - loss: 0.6553 - accuracy: 0.6085 - val_loss: 0.6841 - val_accuracy: 0.5601\n",
      "Epoch 19/25\n",
      " - 8s - loss: 0.6541 - accuracy: 0.6095 - val_loss: 0.6880 - val_accuracy: 0.5016\n",
      "Epoch 20/25\n",
      " - 8s - loss: 0.6525 - accuracy: 0.6166 - val_loss: 0.6865 - val_accuracy: 0.5817\n",
      "Epoch 21/25\n",
      " - 7s - loss: 0.6519 - accuracy: 0.6194 - val_loss: 0.6846 - val_accuracy: 0.5653\n",
      "Epoch 22/25\n",
      " - 8s - loss: 0.6520 - accuracy: 0.6190 - val_loss: 0.6851 - val_accuracy: 0.5607\n",
      "Epoch 23/25\n",
      " - 8s - loss: 0.6511 - accuracy: 0.6212 - val_loss: 0.6829 - val_accuracy: 0.5601\n",
      "Epoch 24/25\n",
      " - 8s - loss: 0.6485 - accuracy: 0.6227 - val_loss: 0.6870 - val_accuracy: 0.5929\n",
      "Epoch 25/25\n",
      " - 7s - loss: 0.6489 - accuracy: 0.6256 - val_loss: 0.6826 - val_accuracy: 0.5594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2513dc7a248>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=32,epochs=25,validation_data =(x_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = vectorizer.transform(list(test['text']))\n",
    "y_pred = clf.predict(test1)\n",
    "samp = pd.read_csv('sample_submission.csv')\n",
    "samp['target'] = y_pred\n",
    "samp.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
